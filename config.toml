# Page settings

baseurl = "https://resume.cameronboy.com/"
languageCode = "en-us"

theme = "orbit"
googleAnalytics = "UA-114358546-1"

# Do not build files for websites. Need them? Set to false
disableKinds=["404","RSS","sitemap"]

[params]

# Meta
    description = "Resume for Cameron Boy"
    author = "Cameron Boy"
    title = "Cameron Boy Info"

# Theme styles

    # The original template comes with 6 colour schemes. You may choose styles below.
    # "styles.css", "styles-2.css", "styles-3.css", "styles-4.css", "styles-5.css", "styles-6.css"
    # are available. Also the source LESS files are included so
    # itâ€™s quick and easy to change the styling and colour scheme.

    styles = "styles-6.css"

# Sidebar sections

    # Profile section
    [params.profile]
        name = "Cameron Boy"
        tagline = "Data Nerd; Python & R enthusiast"
        avatar = "profile.png"

    # Contact section
    [params.contact]
        enable = true

        [[params.contact.list]]
        class = "resume"
        icon = "fa-download"
        url = "https://cameronboy.s3.amazonaws.com/Cameron_Boy_Resume.docx"
        title = "Download this resume"

        [[params.contact.list]]
        class = "email"
        icon = "fa-envelope"
        url = "mailto: camerondavidboy@gmail.com"
        title = "email"

        [[params.contact.list]]
        class = "website"
        icon = "fa-globe"
        url = "https://cameronboy.com"
        title = "cameronboy.com"

        [[params.contact.list]]
        class = "linkedin"
        icon = "fa-linkedin"
        url = "//linkedin.com/in/cameronboy"
        title = "linkedin.cameronboy.com"

        [[params.contact.list]]
        class = "github"
        icon = "fa-github"
        url = "//github.com/cameronboy"
        title = "github.com/cameronboy"

        [[params.contact.list]]
        class = "twitter"
        icon = "fa-twitter"
        url = "//twitter.com/cboyrun"
        title = "@cboyrun"

    # Education section
    [params.education]
        enable = true
        title = "Education"

        [[params.education.list]]
        degree = ""
        college = "Dual Major:"

        [[params.education.list]]
        degree = "BSc in Finance"
        college = "University of Minnesota"
        dates = "2009 - 2013"

        [[params.education.list]]
        degree = "BSc in MIS"
        college = "University of Minnesota"
        dates = "2009 - 2013"

        

    # Languages section
    [params.language]
        enable = false
        title = "Languages"

        [[params.language.list]]
        language = "English"
        level = "Native"

    # Interests section
    [params.interests]
        enable = true
        title = "Interests"

        [[params.interests.list]]
        interest = "Coffee"

        [[params.interests.list]]
        interest = "Weightlifting"

        [[params.interests.list]]
        interest = "MotoGP"

        [[params.interests.list]]
        interest = "Strong IPAs"



# Main body sections

    # You may use markdown in summary, details and intro fields. But don't overdose, it's resume!:)

    # Summary section 
    [params.summary]
        enable = true
        icon = "fa-user"
        title = "Career Profile"
        summary = """Experienced data professional always looking to learn new and creative ways operate. The last couple years have seen a transition from heavy use in python for statistical results work to data engineer. All this to help round-out my knowledge and capabilities.
        """

    # Experiences section
    [params.experiences]
        enable = true
        icon = "fa-briefcase"
        title = "Experience"

        [[params.jobs.list]]
        position = "Data Scientist"
        dates = "April 2022 - Present"
        company = "BI Worldwide"
        details = """First data science function within our data group to make sense of our unstructured data along with structured data to better support client needs and tap into small, incremental improvements in performance.
        \n * ... more to come
        """

        [[params.jobs.list]]
        position = "Data Engineer"
        dates = "December 2020 - April 2022"
        company = "Charter Communications"
        details = """Our team is responsible for quantifying the user experience across our self-service product portals. `SQL` `hiveQL` `presto` `aws` `terraform` `git`
        \n * Calculate the user experience on Charters account portals through measuring api responses, page loads, and other custom metrics (terms) depending on the feature we're measuring performance for. These terms are scaled between `0` and `1`, weight averaged & summed -- giving product developers & owners a concise, relatable number to compare relative to past performance & other features directly.
        \n * Charter captures > 160M events per day, ~140M of them are events relevant to our pipelines, requiring forethought on appropriate index field usage and other efficient data operations: temp-tables, CTEs, aws Athena (presto) or hive (adhoc cluster or standalone cluster).
        \n * Maintain ETL pipelines that utilize HiveQL run on aws-emr clusters through scheduled aws-coordinators to ensure our pre-aggregations are complete before further aggregations are started.
        \n * Pipeline aws infrastructure is defined in terraform modules allowing us to only worry about any job-specific alterations while keeping best-practices regarding tagging/permissions/fleet management etc up to date through inheritance of common modules managed by our aws platform team.
        \n * Templated our pipelines to allow for single-day runs or back filling with addition of one additional parameter. Updates to gitlab CI/CD pipeline, shell script, and the hiveQL were required. This enhancement saves ~ 20 - 40 minutes on average every time a backfill for reprocessing is required.
        """

        [[params.jobs.list]]
        position = "Career skills upgrade"
        dates = "August 2020 - December 2020"
        details = """Time off to focus on career development & gain AWS Solutions Architect certification."""

        [[params.jobs.list]]
        position = "Decision Scientist"
        dates = "December 2018 - August 2020"
        company = "Ibotta, Inc"
        details = """Our team was the analytical and technical support for our client success teams. Our work aims to provide automated, multifaceted campaign results via a multitude of tools include SQL, Spark, Python, and Apache Airflow. `python` `pyspark` `airflow` `presto` `git` `test/holdout audiences`
        \n * Authored pyspark/airflow ETL pipeline to calculate program results, storing results in s3 for nightly ingestion to our snowflake datastore feeding looker dashboards & explores.
        \n * Technical lead on our AB testing pipeline in object-oriented pyspark and airflow to measure statistical effectiveness of Ibotta platform.
        \n * Query optimization liaison for our team, consulting with our team on efficient SQL structure and operations to help reduce run time and compute resources.
        \n * Automated the power analysis to determine optimal control sizes along with stratifying the groups to ensure similar behavior across the treatment & control.
        \n * Ported repeated data analysis scripts into our internal python package for reproducibility and version control.
        \n * Addressed continual support requests with adding functionality to our internal python module for rest of the team to use."""


        [[params.jobs.list]]
        position = "Finance Data Analyst"
        dates = "August 2017 - December 2018"
        company = "Johns Manville"
        details = """I worked to provide a technical background to our data projects with SQL support, advanced tableau, and python on our team within the finance department. Our team operated in close contact with the CFO to be the central analytics hub and to provide advanced data & analytics support to Johns Manville finance. `SQL` `Tableau` `python` `requirements-gathering`
        \n * Introduced python to our team with ETL pipelines to feed our MSSQL database.
        \n * Built a model with keras (tensorflow backend) in python to estimate tax liabilities to bring potential errors to the attention of our tax department thus reducing the possibility of incurring consultant fees.
        \n * Managed analytics projects to completion supporting commercial sales analytics to internal tax reconciliation"""

        [[params.jobs.list]]
        position = "Data Analyst"
        dates = "March 2015 - August 2017"
        company = "BI Worldwide"
        details = """With a team that provided multifaceted analytic support for multiple channels of a large, national-telecom client. We were on top of our descriptive analytics game and worked to apply a more systematic approach to analysis to extract deeper information from our data. 
        \n `SQL` `RStats` `Tableau` `PowerPivot` `requirements-gathering` `data-communication`
        \n * Mined our hosted blog and ran sentiment analyses with R to shed light on open-ended participant feedback.
        \n * Developed a logistic regression model to determine the probability of referrals selling, and experimented with clustering on our program population to delve into promotion results and answer why some groups perform better than others.
        \n * Integrated R into some of our analyses for both internal and external stakeholder with final reporting is rendered in HTML with the knitr and flexdashboard packages allowing for great deal of interactivity between the stakeholder and the data for those without Tableau licenses."""


        [[params.jobs.list]]
        position = "Project Coordinator"
        dates = "September 2014 - March 2015"
        company = "BI Worldwide"
        details = """Our team ran and operated the incentive programs for numerous automobile clients through an in-house developed web app, back-end databases and procedures, reporting, and client support.
        \n * Learned SQL on the job and ported all of our excel reports to SQL, saving our monday mornings.
        \n * Support Business Analysts with fielding of participant inquiries in a timely manner.
        \n * Responsible for development and execution of day-to-day sales reporting distributed to the client.
        \n * Complete documentation of database and Website procedures.
        """


        [[params.jobs.list]]
        position = "Data Analyst"
        dates = "June 2013 - September 2014"
        company = "AmerisourceBergen"
        details = """* Designed, developed, implemented, and tested new processes and dashboards that improved data analysis and tracking, reduced cost, and improved process effectiveness and accuracy.
        \n * Directly oversaw10+ of these analysis processes that monitor our performance on compliance contracts valued at millions in quarterly revenue.
        \n * Consulted with end-users on their requirements to drive my report design, development and testing.
        \n * Communicated with our pharmaceutical buyers on actionable steps towards maximizing our inventory position whilst minimizing penalties associated with erratic purchasing activity.
        \n * Conducted mathematical analysis of inventory/service level data to ensure maximum reward payment from our contracted suppliers.
        """



    # Projects section
    [params.projects]
        enable = true
        icon = "fa-archive"
        title = "Projects"
        intro = "#### Some side projects worked on throughout free time"

        [[params.projects.list]]
        title = "MotoGP Data"
        url = "https://github.com/cameronboy/motogp-analysis"
        tagline = "Dorna publishes MotoGP results lap-by-lap in highly formatted PDFs. I want to take that data and put it in a useable analysis format"

        
        [[params.projects.list]]
        title = "Site Response Monitoring"
        url = "https://github.com/cameronboy/site_response_monitoring"
        tagline = "A python script to test bandwidth to popular content providers and log to Google sheets."
        
        [[params.projects.list]]
        title = "Neural Network in pure numpy - Udacity project"
        url = "https://github.com/cameronboy/DLND-P1/blob/master/Neural%20Network%20-%20Bike%20Demand%20Prediction.ipynb"
        tagline = "A vanilla neural network implemented in pure `numpy` to predict bike sharing demand."
        
        [[params.projects.list]]
        title = 'NLP - Machine Translation - Udacity project'
        url = "https://github.com/cameronboy/DLND-P4/blob/master/dlnd_language_translation.ipynb"
        tagline = 'A sequence to sequence model translating english text to french.'
        
        [[params.projects.list]]
        title = "Sudoku solver - Udacity project"
        url = "https://github.com/cameronboy/aind-sudoku"
        tagline = "An intelligent agent that solves sudoku puzzles."

        [[params.projects.list]]
        title = 'Isolation Agent - Udacity project'
        url = 'https://github.com/cameronboy/aind-isolation'
        tagline = 'An agent utilizing alpha-beta pruning to play isolation.'

    # Skills section
    [params.skills]
        enable = false
        icon = "fa-rocket"
        title = "Skills & Proficiencies"

        [[params.skills.list]]
        skill = "Python"
        level = "90%"

        [[params.skills.list]]
        skill = "aws"
        level = "75%"

        [[params.skills.list]]
        skill = "Airflow"
        level = "75%"

        [[params.skills.list]]
        skill = "DB Normalization"
        level = "75%"


    # Footer section

    # The original template is released under the Creative Commons Attribution 3.0 License.
    # Please keep the original attribution link when using for your own project.
    # If you'd like to use the template without the attribution,
    # you can check out other license options via template author's website: themes.3rdwavemedia.com
    #
    # As for Hugo port you may rewrite the "Ported for..." line with setting your name below.

    [params.footer]
        copyright = "Cameron Boy"
